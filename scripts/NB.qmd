---
title: "NB"
format: html
editor: visual
---

```{r}
library(bnlearn)
library(tidyverse)
```

```{r}
library(tidytext)
library(knitr)
library(stringr)
```

```{r}
texto = read.csv("../data/ghost_stories_1000.csv")
```

```{r}
texto <- texto |>
  mutate(category = str_trim(category),
         category = na_if(category, ""),
         category = if_else(is.na(category), "Unknown", category))

```

```{r}
library(dplyr)

 df <- texto |>
  filter(category %in% c("Haunted Places",
                         "Apparitions / Voices / Touches")) |>
  select(story_id, title, category, story)   

```

```{r}
df <- df %>%
  mutate(story_id = row_number())

write.csv(df, "../data/ghost_stories_filtered.csv", row.names = FALSE)
view(df)
```

```{r}
df |>
  distinct(category)
```

```{r}
df2 <- df %>%
  transmute(doc_id = story_id,
            category = category,
            text = story)
```

```{r}
words <- df2 |>
  unnest_tokens(word, text) |>
  filter(!str_detect(word, "^[0-9]+$"))        
```

```{r}
words |>
  count(category, word, sort = TRUE) |>
  group_by(category) |>
  slice_head(n = 10) |>
  pivot_wider(
    names_from = category, 
    values_from = n,
    values_fn = as.character,
    values_fill = "Not in top 10"
    ) |>
  kable()
```

```{r}
stop_words
```

```{r}
words <- words |>
  anti_join(stop_words, by = "word")
```

```{r}
words |>
  count(category, word, sort = TRUE) |>
  group_by(category) |>
  slice_head(n = 10) |>
  pivot_wider(
    names_from = category, 
    values_from = n,
    values_fn = as.character,
    values_fill = "Not in top 10"
    ) |>
  kable()
```

```{r}
words |>
  count(category, word, sort = TRUE) |>
  group_by(category) |>
  slice_head(n = 10) |>
  ggplot(aes(y = reorder_within(word, n, category), x = n, fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, scales = "free") +
  scale_y_reordered() +
  labs(y = NULL)
```

> Con estas repeticiones de palabras nos podemos dar una idea de como el modelo va a a predecir las categorias, ya que tomaría en cuenta la repetición y conteo de palabras por cada categoría.

Sparse Matrix

```{r}
library(Matrix) 
```

```{r}
tokens <- df2 |>
  unnest_tokens(word, text) |>
  filter(!str_detect(word, "^[0-9]+$")) |>
  anti_join(stop_words, by = "word")
```

```{r}
counts <- tokens |>
  count(doc_id, word, name = "freq")
```

```{r}
X <- cast_sparse(
  counts,
  row = doc_id,
  col = word,
  value = freq
)
```

```{r}
meta <- df2 |>
  distinct(doc_id, category)
```

```{r}
labels <- meta$category[match(as.integer(rownames(X)), meta$doc_id)]
```

Training/Test separation

```{r}
library(rsample)
```

```{r}
set.seed(123)
split <- initial_split(
                      tibble(doc_id = rownames(X),
                            category = labels),
                      prop = 0.8,
                      strata = category)
```

```{r}
train_ids <- training(split)$doc_id
test_ids  <- testing(split)$doc_id
```

```{r}
train_index <- match(train_ids, rownames(X))
test_index  <- match(test_ids, rownames(X))
```

```{r}
X_train <- X[train_index, ]
y_train <- labels[train_index]
```

```{r}
X_test  <- X[test_index, ]
y_test  <- labels[test_index]
```

```{r}
train_df <- as.data.frame(as.matrix(X_train))
train_df$category <- as.factor(y_train)
```



```{r}
library(e1071) 
library(yardstick)
```

```{r}
nb_model <- naiveBayes(category ~ ., data = train_df)
```

```{r}
test_df <- as.data.frame(as.matrix(X_test))
pred <- predict(nb_model, test_df)
```

#### Matriz de Confusión
```{r}
conf_mat <- table(Predicho = pred, Real = y_test)
print(conf_mat)
``` 
Clase real "Apparitions / Voices / Touches"

- 28 veces se predijo correctamente.

- 46 veces fueron mal clasificadas como esa clase cuando en realidad eran "Haunted Places".

Clase real "Haunted Places"

- 0 aciertos, el modelo nunca la predijo.

Esto ya explica el sesgo del clasificador: solo aprende a reconocer la clase mayoritaria.

#### Accuracy
```{r}
accuracy <- mean(pred == y_test)
cat("Accuracy:", accuracy, "\n")
```
Es bajo: el modelo acierta poco más de 1 de cada 3 casos.

#### Métricas por clase

```{r}
classes <- levels(y_test)

get_metrics <- function(k) {
  TP <- conf_mat[k, k]
  FP <- sum(conf_mat[k, ]) - TP
  FN <- sum(conf_mat[, k]) - TP
  P  <- if ((TP + FP) == 0) NA else TP / (TP + FP)
  R  <- if ((TP + FN) == 0) NA else TP / (TP + FN)
  F1 <- if (is.na(P) || is.na(R) || (P + R) == 0) NA else 2 * P * R / (P + R)
  c(Clase = k, Precision = P, Recall = R, F1 = F1)
}

metrics <- as.data.frame(do.call(rbind, lapply(classes, get_metrics)))
metrics$Precision <- as.numeric(metrics$Precision)
metrics$Recall    <- as.numeric(metrics$Recall)
metrics$F1        <- as.numeric(metrics$F1)

print(metrics)
```

Apparitions / Voices / Touches

Precisión (0.37): de todas las veces que el modelo predijo esta clase, solo el 37 % fueron correctas.

Recall (1.0): recuperó todos los casos reales de esta clase.

F1 (0.55): balance intermedio; refleja que aunque cubre todo (recall perfecto), se equivoca mucho al sobrepredecirla.

Haunted Places

Precisión = NA: nunca fue predicha → no hay aciertos.

Recall = 0: había ejemplos reales, pero el modelo no reconoció ninguno.

F1 = NA: no hay balance posible, ya que no predijo nada bien de esta clase.


#### Función naive_bayes()
```{r}
install.packages("naivebayes")
```

```{r}
library(naivebayes)
```

```{r}
nb_model2 <- naive_bayes(category ~ ., data = train_df)
```

```{r}
pred2 <- predict(nb_model2, newdata = test_df)
```

```{r}
conf_mat2 <- table(Predicho = pred2, Real = y_test)
print(conf_mat2)
```

```{r}
accuracy2 <- mean(pred2 == y_test)
cat("Accuracy (naive_bayes):", accuracy2, "\n")
```
No existe ninguna diferencia entre ambos clasificadores.
